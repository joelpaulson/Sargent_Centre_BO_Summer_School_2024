{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["jG4BvD4HBoTu","naj1dX5C0RGh","oBIUPhCSBkFj","E_wDIz7jxr8d","tk9N1RtKBslS","RSsyRnOl8d2K","pyYucFmt5KAf","ofQDtGm7TL7L","yixgKwVVPvhO","kJIrBNlrPxHp","JmK9ib8rC1ce"],"toc_visible":true,"authorship_tag":"ABX9TyN76Sf+s4OfKLnKBjp8J0An"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Gaussian Processes\n","## Hands-On Code Tutorial\n","*   Instructor: Joel Paulson, Department of Chemical and Biomolecular Engineering, The Ohio State University\n","*   Location: Sargent Centre Summer School on Bayesian Optimization\n","*   Date: 09/03/24"],"metadata":{"id":"HBzMkPtpY5Mv"}},{"cell_type":"markdown","source":["# Notebook Overview\n","\n","This notebook provides an introduction to Gaussian process (GP) modeling in Python, using mostly the flexible and scalable Python package [GPyTorch](https://gpytorch.ai/). It is broken up into the following 4 major sections:\n","0. A Quick PyTorch Primer.\n","  * This section is meant to provide a short introduction to key features of PyTorch that we will need throughout the sections.\n","1. GPs from Scratch:\n","  * In this section, we will implement the core components of a Gaussian process from scratch including defining a kernel, drawing GP samples from the prior and posterior, and estimating hyperparamters using the marginal log likelihood function.\n","2. Introduction to GPyTorch\n","  * This section provides a high-level introduction to the GPyTorch package such that you can see how to go about using it to perform basic regression tasks (with multiple input features). It also includes an exercise where you need to define your own custom kernel for the case of Brownian motion.\n","3. GP Regression: Modeling CO$_2$ Levels at the Mauna Loa Observatory\n","  * In this section, you will have the chance to apply the different things you have learned in the previous sections to model the monthly average atmospheric CO$_2$ concentration collected at the Mauna Loa Observatory in Hawaii. This example is based on Section 5.4.3 of the [Gaussian Processes for Machine Learning book](https://gaussianprocess.org/gpml/chapters/RW.pdf) and is interesting as it requires relatively complex kernel engineering to achieve good test (and extrapolation) performance in practice."],"metadata":{"id":"ajDnBqAeZhiH"}},{"cell_type":"markdown","source":["# Setup (Install Packages)\n","\n","Not all the packages that we plan to use come installed in Google Colab, so run the code below to install external packages. Note that the ```&> /dev/null``` part of the code just supresses the output of the bash command."],"metadata":{"id":"k5Zqpagha-oP"}},{"cell_type":"code","source":["!pip install gpytorch &> /dev/null  #add gpytorch (https://gpytorch.ai/)\n","!pip install botorch  &> /dev/null  #add botorch (https://botorch.org/)"],"metadata":{"id":"r1U8iDwcbCHR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import all the necessary packages used throughout this notebook\n","from botorch import fit_gpytorch_mll\n","import gpytorch\n","from gpytorch.constraints import Positive\n","import math\n","import matplotlib.pyplot as plt\n","from matplotlib import cm\n","import numpy as np\n","import scipy.stats as stats\n","from sklearn.datasets import fetch_openml\n","import torch"],"metadata":{"id":"nHohoTwb_U9L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 0. A Quick PyTorch Primer\n","\n","PyTorch is an open-source deep learning framework that provides a flexible and intuitive interface for building and training neural networks. It is particularly well-suited for tensor manipulation and automatic differentiation, making it a popular choice among researchers, developers, and practitioners for a wide range of machine learning tasks."],"metadata":{"id":"t0_D9zTXb8b6"}},{"cell_type":"markdown","source":["## 0.1 Key Features\n","* **Tensor Manipulation:** PyTorch tensors are multi-dimensional arrays similar to NumPy arrays, but with additional capabilities for GPU acceleration. They form the building blocks for all computations in PyTorch and can be used for all sorts of data manipulation. Performing operations on a Tensor level is often at the heart of the efficiency gains that can be achieved in practice.\n","* **Automatic Differentiation:** PyTorch’s autograd module enables automatic computation of gradients, which is essential for training neural networks and Gaussian processes. This feature allows you to easily backpropagate errors and update model parameters.\n","* **Dynamic Computational Graphs:** PyTorch uses dynamic computational graphs (also known as define-by-run), which are constructed on-the-fly during each iteration. This flexibility allows for more complex model architectures and easier debugging."],"metadata":{"id":"75F4-tKdctHi"}},{"cell_type":"markdown","source":["## 0.2 Code Examples\n","\n","For those of you that are unfamiliar with PyTorch, it may be useful to read through the following code blocks for a quick introduction to how Tensor objects are defined and how automatic differentiation (AD) can be executed by calling [```.backward()```](https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html) on a Tensor that triggers differentiation to be carried out backward through the computational graph using chain rule."],"metadata":{"id":"kQUq5hL-dSNf"}},{"cell_type":"markdown","source":["### Tensor Manipulation\n","\n","Let's start by running some basic Tensor operations."],"metadata":{"id":"FslpOPQJe99Q"}},{"cell_type":"code","source":["# fix the random seed\n","torch.manual_seed(42)\n","\n","# define a 2d tensor (matrix) of 3x3 elements by drawing random numbers from a N(0,1)\n","a = torch.randn((3,3))\n","\n","# perform element-wise addition with a number\n","b = a + 1\n","\n","# perform matrix multiplication a * a.T (tranpose)\n","c = a @ a.T\n","\n","# print the results\n","print(\"a is\")\n","print(a)\n","print(\"b is\")\n","print(b)\n","print(\"c is\")\n","print(c)"],"metadata":{"id":"FBkWKgMYdd11"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Automatic Differentiation\n","\n","PyTorch’s ```autograd``` allows for automatic differentiation, which is crucial for optimizing models during training. Below is a simple example of how we can use this feature to compute the partial derivatives of an output $z = x \\cdot y + y^2$ with respect to its two inputs $x$ and $y$ at specific values $x=2$ and $y=3$, i.e., $\\left. \\frac{\\partial z}{\\partial x} \\right|_{x=2,y=3}$ and $\\left. \\frac{\\partial z}{\\partial y} \\right|_{x=2,y=3}$."],"metadata":{"id":"7pO8dkWLfIlf"}},{"cell_type":"code","source":["# create tensors with requires_grad=True to track computation\n","# (otherwise the variable is not tracked in the gradient calculation)\n","x = torch.tensor(2.0, requires_grad=True)\n","y = torch.tensor(3.0, requires_grad=True)\n","\n","# define simple function\n","z = x * y + y**2\n","\n","# compute gradients\n","z.backward()\n","\n","# print results\n","print(f\"dz/dx at x={x.item()} and y={y.item()}:  {x.grad.item()}\")  # Should be y (3.0)\n","print(f\"dz/dy at x={x.item()} and y={y.item()}:  {y.grad.item()}\")  # Should be x + 2*y (2.0 + 6.0 = 8.0)"],"metadata":{"id":"U1WeKLNJfIsR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A special feature of PyTorch is its ability to easily handle \"batching\". Batching refers to processing multiple data points simultaneously rather than one at a time, and is essential for efficient computation. It is especially valuable when working with large datasets and neural networks. In the context of the automatic differentiation code above, batching would implying that we are computing the gradient for multiple $(x,y)$ points of interest at the same time. Let us see how this works:"],"metadata":{"id":"EM5XMRFwgsFp"}},{"cell_type":"code","source":["# create tensors with requires_grad=True to track computation\n","# (otherwise the variable is not tracked in the gradient calculation)\n","x = torch.tensor([2.0, 4.0, 6.0], requires_grad=True)  # batch of x values\n","y = torch.tensor([3.0, 5.0, 7.0], requires_grad=True)  # batch of y values\n","\n","# define simple function\n","z = x * y + y**2\n","\n","# compute gradients\n","#   .backward() only works natively on a scalar function, so we need to pass an\n","#   additional tensor to specify how to combine these elements into a single\n","#   scalar value to compute the gradients --> torch.ones_like(z) effectively\n","#   equally sums up the elements in the batch before calling .backward()\n","z.backward(torch.ones_like(z))\n","\n","# print results\n","print(f\"dz/dx at batch of (x,y) values:  {x.grad.detach().numpy()}\")\n","print(f\"dz/dy at batch of (x,y) values:  {y.grad.detach().numpy()}\")"],"metadata":{"id":"9rZBMqH_gsML"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1. GPs from Scratch\n","\n","In the presentation slides, we saw that a Gaussian process (GP) is a collection of random variables, any finite number of which have a joint Gaussian distribution. GPs are typically denoted by\n","\n","$$\n","f(\\mathbf{x}) \\sim GP(\\mu(\\mathbf{x}), k(\\mathbf{x}, \\mathbf{x}'))\n","$$\n","\n","where $\\mu(\\mathbf{x})$ is the mean function and $k(\\mathbf{x}, \\mathbf{x}')$ is the kernel function (equal to the covariance of the function values $f(\\mathbf{x})$ and $f(\\mathbf{x}')$ at the two points $\\mathbf{x}$ and $\\mathbf{x}'$). We will mostly assume the mean function is zero, i.e., $\\mu(\\mathbf{x} = 0)$ for convenience.\n","\n","In the sections below, we want to build our own simple implementation of a GP with a radial basis function (RBF) (also known as squared exponential (SE)) kernel using standard PyTorch operations. These exercises are meant to provide more insight into the inner workings of a GP. Specifically, we will perform the following tasks:\n","* 1.1: Build our own implementation of the RBF kernel in PyTorch\n","* 1.2: Draw independent random samples from a GP prior\n","* 1.3: Using the Gaussian condtional formula to draw independent random samples from a GP posterior\n","* 1.4: Use the maginal log likelihod (MLL) to identify good kernel hyperparameters that are able to match observed data."],"metadata":{"id":"wFgnvMPMZqh4"}},{"cell_type":"markdown","source":["## 1.1 RBF Kernel\n","\n","Recall that the RBF kernel has the following form\n","\n","$$\n","k_\\text{RBF}(\\mathbf{x}, \\mathbf{x}') = a^2 \\exp\\left(-\\frac{\\| \\mathbf{x} - \\mathbf{x}' \\|^2}{2 l^2} \\right)\n","$$\n","\n","where $a$ is a hyperparameter related to the scale of the function, $l$ is the lengthscale that controls the correlation length (note we have assumed a single, common lengthscale for all dimensions for simplicity), and $\\| \\cdot \\|$ denotes the standard Euclidean norm."],"metadata":{"id":"9kutTGdHkFiW"}},{"cell_type":"markdown","source":["### Exercise\n","\n","**TASK:** Your goal is to implement a Python function that evaluates the RBF kernel between two sets of points. The structure of the function (with all inputs and outputs) is provided in the code block below -- your goal is to complete the function. Note that your function needs to support batching, so I recommend that you use the [```torch.cdist```](https://pytorch.org/docs/stable/generated/torch.cdist.html) function to compute $\\| \\mathbf{x}_1 - \\mathbf{x}_2 \\|$ for a batch of inputs."],"metadata":{"id":"jG4BvD4HBoTu"}},{"cell_type":"code","source":["# define the RBF kernel function\n","def rbf_kernel(x1, x2, l=1.0, a=1.0):\n","    \"\"\"\n","    Computes the RBF kernel between two sets of points.\n","\n","    Parameters\n","    ----------\n","    x1 : torch.Tensor\n","        First set of points.\n","    x2 : torch.Tensor\n","        Second set of points.\n","    l : float\n","        Lengthscale parameter.\n","    a : float\n","        Scale parameter.\n","\n","    Returns\n","    -------\n","    torch.Tensor\n","        Kernel matrix.\n","    \"\"\"\n","    ### FILL IN\n","\n","# define hyperparameters\n","l = 1.0\n","a = 1.0\n","\n","# create a grid of points in 1d\n","x = torch.linspace(-2, 2, 101)\n","\n","# create a meshgrid for plotting purposes\n","X, Y = torch.meshgrid(x, x)\n","\n","# evaluate covariance function between all elements of x\n","with torch.no_grad():\n","  K = rbf_kernel(x, x, l, a)\n","K = K.detach().numpy()\n","\n","# plot covariance between x vector\n","fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n","c = ax.pcolor(X, Y, K, cmap=cm.Blues)\n","ax.invert_xaxis()\n","ax.set_title(f'a = {a} and l = {l}')\n","fig.colorbar(c, ax=ax)\n","plt.rcParams.update({'font.size':14});"],"metadata":{"id":"VmUeWM4P6LWO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Solution"],"metadata":{"id":"naj1dX5C0RGh"}},{"cell_type":"code","source":["# define the RBF kernel function\n","def rbf_kernel(x1, x2, l=1.0, a=1.0):\n","    \"\"\"\n","    Computes the RBF kernel between two sets of points.\n","\n","    Parameters\n","    ----------\n","    x1 : torch.Tensor\n","        First set of points.\n","    x2 : torch.Tensor\n","        Second set of points.\n","    l : float\n","        Lengthscale parameter.\n","    a : float\n","        Scale parameter.\n","\n","    Returns\n","    -------\n","    torch.Tensor\n","        Kernel matrix.\n","    \"\"\"\n","    # convert to 2d tensors if only 1d provided\n","    if len(x1.shape) == 1:\n","        x1 = x1.unsqueeze(-1)\n","    if len(x2.shape) == 1:\n","        x2 = x2.unsqueeze(-1)\n","\n","    # compute the squared Euclidean distance between the points\n","    sqdist = torch.cdist(x1, x2)**2\n","\n","    # compute the RBF kernel\n","    return a**2 * torch.exp(-sqdist / (2 * l**2))\n","\n","# define hyperparameters\n","l = 1.0\n","a = 1.0\n","\n","# create a grid of points in 1d\n","x = torch.linspace(-2, 2, 101)\n","\n","# create a meshgrid for plotting purposes\n","X, Y = torch.meshgrid(x, x)\n","\n","# evaluate covariance function between all elements of x\n","with torch.no_grad():\n","  K = rbf_kernel(x, x, l, a)\n","K = K.detach().numpy()\n","\n","# plot covariance between x vector\n","fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n","c = ax.pcolor(X, Y, K, cmap=cm.Blues)\n","ax.invert_xaxis()\n","ax.set_title(f'a = {a} and l = {l}')\n","fig.colorbar(c, ax=ax)\n","plt.rcParams.update({'font.size':14});"],"metadata":{"id":"phlmD37S0Qk8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1.2 GP Prior Samples\n","\n","Now that we have the ability to evaluate the RBF kernel over a batch of input values, we want to use this capability to generate random sample GP paths. Recall from the presentation slides that we can always apply the whitening transformation to generate samples from a multivariate normal distribution. In other words, the following distributions are equivalent\n","\n","$$\n","\\mathbf{f} \\sim N(\\mathbf{m}, \\mathbf{K})~~~ <=> ~~~ \\mathbf{f} = \\mathbf{m} + \\mathbf{L}\\mathbf{Z},\n","$$\n","\n","where $\\mathbf{K} = \\mathbf{L}\\mathbf{L}^\\top$ such that $\\mathbf{L}$ is the lower triangular Cholesky factor of the covariance matrix $\\mathbf{K}$ and $\\mathbf{Z} \\sim N(0, I)$ is a set of independent standard normal random variables. Using this idea, generate 3 independent samples of a GP with zero mean and an RBF kernel with scale $a=1$ and lengthscale $l=1$ over a range of $x \\in [-10, 10]$. Note that you can use the following two functions from the torch library:\n","* [```torch.linalg.cholesky(A)```](https://pytorch.org/docs/stable/generated/torch.linalg.cholesky.html) to compute the Cholesky decomposition of a matrix $A$.\n","* [```torch.randn((n,m))```](https://pytorch.org/docs/stable/generated/torch.randn.html) to generate an $n$ by $m$ matrix of standard normal random variables."],"metadata":{"id":"RcY_o6F4kKjF"}},{"cell_type":"markdown","source":["### Exercise\n","\n","**TASK:** Fill in the missing parts of the code block below to generate a plot that shows (i) the 3 independent samples from the GP prior and (ii) the 95\\% confidence region as a gray shaded region. Note that we can use the [scipy.stats](https://docs.scipy.org/doc/scipy/reference/stats.html) package to calculate the scaling factor to compute the lower and upper confidence region for a normal random variable, i.e., $Y \\sim N(\\mu_Y, \\sigma_Y^2)$ will lie in the region\n","\n","$$[\\mu_Y + \\sigma_Y\\Phi^{-1}(0.025), \\mu_Y + \\sigma_Y\\Phi^{-1}(0.975)]$$\n","\n","with 95% probability where $\\Phi^{-1}(p)$ is the inverse CDF for a standard normal random variable evaluated at probability level $p$.\n","* NOTE: You may run into issues with the Cholesky decomposition, as it can only be run on positive definite matrices and there can be numerical issues with the matrix is close to singular. A simple trick to fix this issue is to add some \"jitter\" (a small constant) to the diagonal. I recommend a value of ```1e-5*torch.eye(n)``` as a default jitter, though this can be increased slightly if needed."],"metadata":{"id":"oBIUPhCSBkFj"}},{"cell_type":"code","source":["# fix the random seed\n","torch.manual_seed(42)\n","\n","# create a list of points from [-10,10]\n","n = 101\n","x = torch.linspace(-10, 10, n)\n","\n","# evaluate the rbf kernel at x for the specified hyperparamter values\n","### FILL IN\n","\n","# calculate the Cholesky decomposition\n","### FILL IN\n","\n","# create a plot\n","fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n","\n","# add the 95% confidence region to plot\n","sigma = K.diag().sqrt()\n","lq = sigma*stats.norm.ppf(0.025)\n","uq = sigma*stats.norm.ppf(0.975)\n","plt.plot(x, torch.zeros(n), '-b', linewidth=3)\n","plt.fill_between(x, lq, uq, color='gray', alpha=0.25)\n","\n","# add three sample realizations to plot\n","### FILL IN\n","\n","# set limits, labels, and font size\n","plt.xlim([-10,10])\n","plt.ylim([-4,4])\n","plt.xlabel('input, x')\n","plt.ylabel('output, f(x)')\n","plt.rcParams.update({'font.size':14});"],"metadata":{"id":"jiwYNUCF7JD2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Solution"],"metadata":{"id":"E_wDIz7jxr8d"}},{"cell_type":"code","source":["# fix the random seed\n","torch.manual_seed(42)\n","\n","# create a list of points from [-10,10]\n","n = 101\n","x = torch.linspace(-10, 10, n)\n","\n","# evaluate the rbf kernel at x for the specified hyperparamter values\n","with torch.no_grad(): # add no_grad decorator since we do not need the gradient\n","    K = rbf_kernel(x, x, l=1, a=1)\n","\n","# calculate the Cholesky decomposition\n","L = torch.linalg.cholesky(K + 1e-5*torch.eye(n))\n","\n","# create a plot\n","fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n","\n","# add the 95% confidence region to plot\n","sigma = K.diag().sqrt()\n","lq = sigma*stats.norm.ppf(0.025)\n","uq = sigma*stats.norm.ppf(0.975)\n","plt.plot(x, torch.zeros(n), '-b', linewidth=3)\n","plt.fill_between(x, lq, uq, color='gray', alpha=0.25)\n","\n","# add three sample realizations to plot\n","for i in range(3):\n","  f = torch.matmul(L, torch.randn((n,1)))\n","  plt.plot(x.detach().numpy(), f.detach().numpy(), '-', linewidth=1)\n","\n","# set limits, labels, and font size\n","plt.xlim([-10,10])\n","plt.ylim([-4,4])\n","plt.xlabel('input, x')\n","plt.ylabel('output, f(x)')\n","plt.rcParams.update({'font.size':14});"],"metadata":{"id":"Cuo5v1K0xsGd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1.3 GP Posterior Samples\n","\n","We now want to test out our ability to generate samples from the posterior GP conditioned on observations. We will assume the observations do not have any noise. Therefore, the main result we need to use is the conditional Gaussian formula, which can be summarized as\n","\n","$$\n","\\begin{bmatrix}\n","X \\\\\n","Y\n","\\end{bmatrix} \\sim N\\left( \\begin{bmatrix} a \\\\ b \\end{bmatrix}, \\begin{bmatrix} A &C \\\\ C^\\top &B \\end{bmatrix} \\right)\n","$$\n","\n","leads to\n","\n","$$\n","X | (Y=y) = N( a + C B^{-1} (y - b), A - C B^{-1} C^\\top )\n","$$\n","\n","For the GP posterior case, the variables $X$ correspond to the function values at the test locations $f(x_{1*}), \\ldots, f(x_{t*})$ and the variables $Y$ correspond to the function values at the training locations $f(x_1), \\ldots, f(x_n)$ for $t$ testing points and $n$ training points.\n","\n","First, let us assume the following training points exist:\n","\n","$$\n","x_1 = -6, ~~~ y_1 = 2, \\\\\n","x_2 = -4, ~~~ y_2 = 1, \\\\\n","~~x_3= 3, ~~~~~ y_3 = -1, \\\\\n","$$\n","\n","and that the data is generated from a GP with RBF kernel under hyperparameters $l=1$ and $a=1$."],"metadata":{"id":"7R9Kivw_kSsI"}},{"cell_type":"markdown","source":["### Exercise\n","\n","**TASK:** You want to calculate the GP posterior by generating the mean and covariance matrix for the vector of testing and training values $( f(x_{1*}), \\ldots, f(x_{t*}), f(x_1), f(x_2), f(x_3) )$ and running the conditional formula above -- the test points correspond to a grid of $x$ values between -10 to 10 as before. Since you need all the different blocks (covariance between test-test, test-train, and train-train), I recommend you run these calculations separately to get each of the sub-blocks of the overall covariance matrix. Fill in the missing parts of the code block below to generate a plot that shows (i) the 3 independent samples from the GP posterior and (ii) the 95\\% confidence region as a gray shaded region.\n","* NOTE: You can use ```torch.inverse(B)``` to compute the inverse of matrix $B$ for simplicity since we have a small training set; however, in general, you would want to use more efficient strategies that do not require explicitly computing and storing the inverse."],"metadata":{"id":"tk9N1RtKBslS"}},{"cell_type":"code","source":["# fix the random seed\n","torch.manual_seed(42)\n","\n","# create a list of 101 test points from [-10,10]\n","n = 101\n","x_test = torch.linspace(-10, 10, n)\n","\n","# create training data given in problem statement\n","### FILL IN\n","\n","# need to evaluate the rbf kernel between different train, test combinations\n","### FILL IN\n","\n","# now we can run the conditional calculation since means are 0 by assumption\n","### FILL IN\n","\n","# we can then generate samples using the same whitening transformation idea as before\n","# but now we also need to track the mean function\n","### FILL IN\n","\n","# create a plot\n","fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n","\n","# add the 95% confidence region, where we now need to account for the mean\n","### FILL IN\n","### Make sure to correctly compute mu, lq, and uq\n","plt.plot(x, mu, '-b', linewidth=3)\n","plt.fill_between(x, lq, uq, color='gray', alpha=0.25)\n","\n","# add three sample realizations\n","### FILL IN\n","\n","# set limits, labels, and font size\n","plt.xlim([-10,10])\n","plt.ylim([-4,4])\n","plt.xlabel('input, x')\n","plt.ylabel('output, f(x)')\n","plt.rcParams.update({'font.size':14});"],"metadata":{"id":"jU14mNEU8dqv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Solution"],"metadata":{"id":"RSsyRnOl8d2K"}},{"cell_type":"code","source":["# fix the random seed\n","torch.manual_seed(42)\n","\n","# create a list of 101 test points from [-10,10]\n","n = 101\n","x_test = torch.linspace(-10, 10, n)\n","\n","# create training data given in problem statement\n","x_train = torch.tensor([-6.0, -4.0, 3.0])\n","y_train = torch.tensor([2.0, 1.0, -1.0])\n","\n","# need to evaluate the rbf kernel between different train, test combinations\n","a, l = 1, 1\n","K_test_test = rbf_kernel(x_test, x_test, l=l, a=a)\n","K_train_test = rbf_kernel(x_train, x_test, l=l, a=a)\n","K_train_train = rbf_kernel(x_train, x_train, l=l, a=a)\n","\n","# now we can run the conditional calculation since means are 0 by assumption\n","Kinv_train_train = torch.inverse(K_train_train)\n","posterior_mean = K_train_test.T @ Kinv_train_train @ y_train\n","posterior_cov = K_test_test - K_train_test.T @ Kinv_train_train @ K_train_test\n","\n","# we can then generate samples using the same whitening transformation idea as before\n","# but now we also need to track the mean function\n","mu = posterior_mean\n","L = torch.linalg.cholesky(posterior_cov + 1e-5*torch.eye(n))\n","\n","# create a plot\n","fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n","\n","# add the 95% confidence region, where we now need to account for the mean\n","sigma = posterior_cov.diag().sqrt()\n","lq = mu + sigma*stats.norm.ppf(0.025)\n","uq = mu + sigma*stats.norm.ppf(0.975)\n","plt.plot(x_test, mu, '-b', linewidth=3)\n","plt.fill_between(x_test, lq, uq, color='gray', alpha=0.25)\n","\n","# add three sample realizations\n","for i in range(3):\n","  f = mu.reshape((-1,1)) + torch.matmul(L, torch.randn((n,1)))\n","  plt.plot(x_test.detach().numpy(), f.detach().numpy(), '-', linewidth=1)\n","\n","# set limits, labels, and font size\n","plt.xlim([-10,10])\n","plt.ylim([-4,4])\n","plt.xlabel('input, x')\n","plt.ylabel('output, f(x)')\n","plt.rcParams.update({'font.size':14});"],"metadata":{"id":"ifTf7pg_8eAx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1.4 Find the Hyperparameters\n","\n","In practice, we often do not know the kernel hyperparameters $a$ and $l$ such that they must be learned/tuned given data. As discussed in the presentation slides, this tuning process can be done using the marginal log likelihood (MLL). For a GP prior $\\mathbf{f} \\sim N(0, \\mathbf{K}_\\theta)$ (evaluated at the training data points $\\mathbf{X}$) with hyperparameters $\\theta = (a,l)$ and a Gaussian likelihood with independent noise realizations $\\mathbf{y} \\mid \\mathbf{f} \\sim N(\\mathbf{f}, \\sigma_n^2 I)$ (with $\\sigma_n^2$ denoting the noise variance), we can derive the following exact expression for the MLL:\n","\n","$$\n","\\log p( \\mathbf{y} | \\mathbf{X}, \\theta, \\sigma_n^2 ) = -\\frac{1}{2} \\mathbf{y}^\\top (\\mathbf{K}_\\theta + \\sigma_n^2 I )^{-1} \\mathbf{y} - \\frac{1}{2} \\log\\det( \\mathbf{K}_\\theta + \\sigma_n^2 I ) - \\frac{n}{2}\\log(2\\pi)\n","$$\n","\n","where $n$ denotes the number of training datapoints. Although we could learn $\\sigma_n^2$ along with $\\theta$, here we will assume that the observation noise is fixed and small $\\sigma_n^2 = 1 \\times 10^{-4}$, meaning it effectively acts like a \"jitter\" in the covariance matrix.\n","\n","Typically, one select the \"optimal\" hyperparameters as the ones that maximize the MLL, i.e., $\\theta^* = \\text{argmax}_{\\theta} \\log p( \\mathbf{y} | \\mathbf{X}, \\theta, \\sigma_n^2 )$ for a particular training dataset $\\{ \\mathbf{y}, \\mathbf{X} \\}$. Here, we want to do a similar procedure but not rigirously -- just a trial-and-error evaluation of the MLL to see if we can find settings that look like they match the \"real\" data generating process.\n","\n","For simplicity, we have packaged the GP prior and posterior calculations that we wrote previously into helper functions that can be obtained by running the hidden code block below. The next code block shows how these functions can be used."],"metadata":{"id":"9WNXhajGkcGP"}},{"cell_type":"code","source":["# @title Helper Functions\n","# create helper function that can evaluate and plot prior predictive equations for GP\n","def GP_prior_predictive(x_test, l, a, sigma2_noise=1e-5, n_samples=3, plot_on=True):\n","    # evaluate the rbf kernel at x for the specified hyperparamter values\n","    with torch.no_grad(): # add no_grad decorator since we do not need the gradient\n","        K = rbf_kernel(x_test, x_test, l=l, a=a)\n","\n","    # prior mean at the text locations\n","    n = x_test.shape[0]\n","    prior_mean = torch.zeros((n,1))\n","\n","    # calculate the Cholesky decomposition\n","    mu = prior_mean\n","    L = torch.linalg.cholesky(K + sigma2_noise*torch.eye(n))\n","\n","    # calculate samples\n","    gp_prior_samples = torch.zeros((n,n_samples))\n","    for i in range(n_samples):\n","        gp_prior_samples[:,i] = torch.matmul(L, torch.randn((n,1))).squeeze()\n","\n","    if plot_on:\n","        # create a plot\n","        fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n","\n","        # add the 95% confidence region to plot\n","        sigma = K.diag().sqrt()\n","        lq = sigma*stats.norm.ppf(0.025)\n","        uq = sigma*stats.norm.ppf(0.975)\n","        plt.plot(x_test, torch.zeros(n), '-b', linewidth=3)\n","        plt.fill_between(x_test, lq, uq, color='gray', alpha=0.25)\n","\n","        # add three sample realizations to plot\n","        for i in range(n_samples):\n","            plt.plot(x_test.detach().numpy(), gp_prior_samples[:,i].detach().numpy(), '-', linewidth=1)\n","\n","        # set limits, labels, and font size\n","        plt.xlim([-10,10])\n","        plt.ylim([-4,4])\n","        plt.xlabel('input, x')\n","        plt.ylabel('output, f(x)')\n","        plt.rcParams.update({'font.size':14});\n","\n","    # return mean, covariance, and samples\n","    return prior_mean, K, gp_prior_samples\n","\n","# create helper function that can evaluate and plot posterior predictive equations for GP\n","def GP_posterior_predictive(x_test, x_train, y_train, l, a, sigma2_noise=1e-5, n_samples=3, plot_on=True):\n","    # need to evaluate the rbf kernel between different train, test combinations\n","    with torch.no_grad(): # add no_grad decorator since we do not need the gradient\n","        K_test_test = rbf_kernel(x_test, x_test, l=l, a=a)\n","        K_train_test = rbf_kernel(x_train, x_test, l=l, a=a)\n","        K_train_train = rbf_kernel(x_train, x_train, l=l, a=a)\n","\n","    # now we can run the conditional calculation since means are 0 by assumption\n","    L = torch.linalg.cholesky(K_train_train)  # K_train_train = L * L.T\n","    alpha = torch.cholesky_solve(y_train.unsqueeze(-1), L)  # Solves L * L.T * alpha = y_train\n","    posterior_mean = K_train_test.T @ alpha\n","    v = torch.cholesky_solve(K_train_test, L)  # Solves L * v = K_train_test\n","    posterior_cov = K_test_test - K_train_test.T @ v\n","\n","    # calculate cholesky factor for test data\n","    n = x_test.shape[0]\n","    mu = posterior_mean.squeeze()\n","    L = torch.linalg.cholesky(posterior_cov + sigma2_noise*torch.eye(n))\n","\n","    # calculate samples from posterior\n","    gp_posterior_samples = torch.zeros((n,n_samples))\n","    for i in range(n_samples):\n","        gp_posterior_samples[:,i] = (mu.reshape((-1,1)) + torch.matmul(L, torch.randn((n,1)))).squeeze()\n","\n","    if plot_on:\n","        # create a plot\n","        fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n","\n","        # add the 95% confidence region, where we now need to account for the mean\n","        sigma = torch.max(posterior_cov.diag(),torch.tensor([0.])).sqrt()\n","        lq = mu + sigma*stats.norm.ppf(0.025)\n","        uq = mu + sigma*stats.norm.ppf(0.975)\n","        plt.plot(x_test, mu, '-b', linewidth=3)\n","        plt.fill_between(x_test, lq, uq, color='gray', alpha=0.25)\n","\n","        # add three sample realizations\n","        for i in range(n_samples):\n","            plt.plot(x_test.detach().numpy(), gp_posterior_samples[:,i].detach().numpy(), '-', linewidth=1)\n","\n","        # add training points to the plot\n","        plt.scatter(x_train, y_train, s=200, color='k', marker=\"+\", zorder=5)\n","\n","        # set limits, labels, and font size\n","        plt.xlim([-10,10])\n","        factor = torch.ceil(torch.max(torch.abs(torch.min(lq)), torch.max(uq)))\n","        plt.ylim([-factor-2,factor+2])\n","        plt.xlabel('input, x')\n","        plt.ylabel('output, f(x)')\n","        plt.rcParams.update({'font.size':14});\n","\n","    # return the mean and covariance\n","    return posterior_mean, posterior_cov, gp_posterior_samples"],"metadata":{"cellView":"form","id":"JDeYfIhSY3Fj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test and train points\n","x_test = torch.linspace(-10, 10, 101)\n","x_train = torch.tensor([-6.0, -4.0, 3.0])\n","y_train = torch.tensor([2.0, 1.0, -1.0])\n","\n","# run operations on prior\n","torch.manual_seed(42)\n","prior_mean, prior_cov, prior_sample_paths = GP_prior_predictive(x_test, l=1, a=1, sigma2_noise=1e-5, n_samples=3, plot_on=True);\n","\n","# run operations on posterior\n","torch.manual_seed(42)\n","post_mean, post_cov, post_sample_paths = GP_posterior_predictive(x_test, x_train, y_train, l=1, a=1, sigma2_noise=1e-5, n_samples=3, plot_on=True);"],"metadata":{"id":"GpBWU7FtTL1P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Exercise\n","\n","**TASK:** Fill in the missing parts of the code block below to evaluate the MLL matching the assumptions listed above (GP prior with zero mean and RBF kernel) for specific hyperparamter $\\theta$ values. The data is pre-loaded below. To help, we have packaged the code that we wrote in sections 1.2 and 1.3 into helper functions that you can create by running the hidden code block above."],"metadata":{"id":"pyYucFmt5KAf"}},{"cell_type":"code","source":["# observed data\n","x_train = torch.Tensor([-8.0000, -6.2222, -4.4444, -2.6667, -0.8889,  0.8889,  2.6667,  4.4444,   6.2222,  8.0000])\n","y_train = torch.Tensor([-1.1544, -0.9582,  0.3840,  1.0536,  0.2901, -0.6560, -0.9193, -0.2800,   0.3737,  0.5213])\n","\n","# create a function that evaluates the MLL given y and K\n","### FILL IN\n","\n","# create test points at which to evaluate the\n","x_test = torch.linspace(-10, 10, 101)\n","\n","# create a list of possible lengthscale values\n","l_list = torch.linspace(0.2, 4, 101)\n","\n","# evaluate the mll for all values in this list\n","a_true = 1\n","sigma2_noise = 1e-4\n","mll_list = torch.zeros(len(l_list))\n","for i in range(len(l_list)):\n","    ### FILL IN\n","    ### Evaluate the MLL at the different lengthscales in l_list\n","    mll_list[i] =\n","\n","# create a plot of lengthscale values vs MLL\n","fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n","plt.plot(l_list, mll_list, '-b', linewidth=3)\n","plt.xlabel('lengthscale, l')\n","plt.ylabel('MLL')\n","plt.rcParams.update({'font.size':14})"],"metadata":{"id":"9fpoycrZa-Tw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Solution"],"metadata":{"id":"ofQDtGm7TL7L"}},{"cell_type":"code","source":["# observed data\n","x_train = torch.Tensor([-8.0000, -6.2222, -4.4444, -2.6667, -0.8889,  0.8889,  2.6667,  4.4444,   6.2222,  8.0000])\n","y_train = torch.Tensor([-1.1544, -0.9582,  0.3840,  1.0536,  0.2901, -0.6560, -0.9193, -0.2800,   0.3737,  0.5213])\n","\n","# create a function that evaluates the MLL given y and K\n","def GP_MLL(y, K, sigma2_noise=1e-4):\n","    n = y.shape[0]\n","    y = y.reshape((-1,1))\n","    Ky = K + sigma2_noise*torch.eye(n)\n","    mll = -0.5*y.T @ torch.inverse(Ky) @ y - 0.5*torch.logdet(Ky) - n/2*np.log(2*np.pi)\n","    return mll.item()\n","\n","# create test points at which to evaluate the\n","x_test = torch.linspace(-10, 10, 101)\n","\n","# create a list of possible lengthscale values\n","l_list = torch.linspace(0.2, 4, 101)\n","\n","# evaluate the mll for all values in this list\n","a_true = 1\n","sigma2_noise = 1e-4\n","mll_list = torch.zeros(len(l_list))\n","for i in range(len(l_list)):\n","    mu_prior, K_prior, sample_paths_prior = GP_prior_predictive(x_train, l=l_list[i], a=a_true, sigma2_noise=sigma2_noise, n_samples=0, plot_on=False);\n","    mll_list[i] = GP_MLL(y_train, K_prior, sigma2_noise=sigma2_noise)\n","\n","# create a plot of lengthscale values vs MLL\n","fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n","plt.plot(l_list, mll_list, '-b', linewidth=3)\n","plt.xlabel('lengthscale, l')\n","plt.ylabel('MLL')\n","plt.rcParams.update({'font.size':14})\n","### TRUE ANSWER: l = 2.5"],"metadata":{"id":"9QOngH4zVAZL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can see that the optimal value that maximizes the MLL is around 2.4 to 2.6, which is close to the real value of 2.5. Note that this will not always be the case, as it depends on the details of the dataset and the structure of the model. As seen in the slides, it is possible to overfit a model when maximizing the MLL, especially when you have a large number of hyperparameters that are being estimated."],"metadata":{"id":"FSVUewRr5S4M"}},{"cell_type":"markdown","source":["# 2. Introduction to GPyTorch\n","\n","GPyTorch is a Python library built on top of PyTorch that provides a scalable and flexible framework for Gaussian Processes (GPs). Gaussian Processes are a powerful non-parametric method for regression, classification, and other machine learning tasks, particularly useful for modeling uncertainty and making predictions with probabilistic interpretations. It leverages the computational efficiency of PyTorch, allowing for the easy integration of Gaussian Processes into deep learning workflows. It is especially well-suited for tasks that require scalable GP models, such as working with large datasets or building complex GP models like deep kernel learning.\n","\n","The sections below are meant to introduce you how to use core components of GPyTorch. Specifically, we will:\n","* 2.1: See how to define a model and likelihood function.\n","* 2.2: Learn how to train a model using torch optimizers.\n","* 2.3: See how to make predictions with a trained model (perform inference).\n","* 2.4: Create a custom kernel object.\n","<!-- * 2.4: Use the default GP training approach in BoTorch as an alternative.  -->\n","\n","Before getting started, make sure you have GPyTorch and BoTorch installed (under the Setup tab)."],"metadata":{"id":"071V1XQ0Z4eu"}},{"cell_type":"markdown","source":["## 2.1 Defining a Model\n","\n","The basic features of a model are summarized in the documentation [here](https://docs.gpytorch.ai/en/v1.6.0/examples/01_Exact_GPs/Simple_GP_Regression.html). The design philosophy behind GPyTorch is that a user is provided with the tools necessary to quickly construct a GP model (as opposed to coming with a pre-packaged model). The rationale is that it is often important for a user to have flexibility to include whatever components are necessary.\n","\n","For most GP regression models, you will need to construct the following GPyTorch objects:\n","* A **GP Model** (```gpytorch.models.ExactGP```) - This handles most of the inference.\n","* A **Likelihood** function (```gpytorch.likelihoods.GaussianLikelihood```) - This is the most common likelihood used for GP regression.\n","* A **Mean** function - Defines the prior mean of the GP. (A ```gpytorch.means.ConstantMean()``` is usually a good place to start.)\n","* A **Kernel** function - Defines the prior covariance of the GP. (e.g., ```gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())``` provides the RBF kernel with a constant scaling factor).\n","* A **MultivariateNormal** Distribution (```gpytorch.distributions.MultivariateNormal```) - This is the object used to represent multivariate normal distributions.\n","\n","To define a model, we typically start by subclassing an existing \"base\" model in GPyTorch. You can find a complete list of models [here](https://docs.gpytorch.ai/en/stable/models.html). For our purposes, we will focus on the exact inference procedure. The main things we need to specify are then:\n","* An **```__init__```** method that takes the training data and a likelihood, and constructs whatever objects are necessary for the model’s forward method. This will most commonly include things like a mean module and a kernel module.\n","* A **```forward```** method that takes in $n \\times d$ data ```x``` and returns a ```MultivariateNormal``` with the prior mean and covariance evaluated at ```x```. In other words, we return the vector $\\mu(x)$ and the $n\\times n$ matrix $K_{xx}$ representing the prior mean and covariance matrix of the GP."],"metadata":{"id":"jw-4wGdJNz9W"}},{"cell_type":"code","source":["# define exact GP model (called MyGP) with constant mean and RBF kernel\n","class MyGP(gpytorch.models.ExactGP):\n","    def __init__(self, train_x, train_y, likelihood):\n","        super(MyGP, self).__init__(train_x, train_y, likelihood)\n","        self.mean_module = gpytorch.means.ConstantMean()\n","        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n","\n","    def forward(self, x):\n","        mean_x = self.mean_module(x)\n","        covar_x = self.covar_module(x)\n","        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"],"metadata":{"id":"2zx--TD2Ktm4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To actually create a model object, we need to pass in some training data. Let's generate a simple set of data from a sin function but include some degree of Gaussian noise in the observations. We will specify a Gaussian likelihood as well."],"metadata":{"id":"0KcgZwsxMoCQ"}},{"cell_type":"code","source":["# generate a set of training points\n","torch.manual_seed(42)\n","train_x = torch.linspace(0, 1, 50)\n","train_y = torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.1\n","\n","# initialize likelihood and model\n","likelihood = gpytorch.likelihoods.GaussianLikelihood()\n","model = MyGP(train_x, train_y, likelihood)"],"metadata":{"id":"etC3h0x5MoIR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Hyperparameters\n","\n","Whenever you create a model, it is likely to have some parameters attached to it where \"parameters\" refer to the objects of type ```torch.nn.Parameter``` that have gradients automatically filled in by the autograd functionality discussed previously in this notebook. In the context of GP regression, these are the hyperparameters of the kernel and/or likelihood function. We can explicitly look at the full set of parameters attached to a model using the ```model.named_parameters()``` generator, as shown in the following code block."],"metadata":{"id":"9KQ120Z3Om1S"}},{"cell_type":"code","source":["for param_name, param in model.named_parameters():\n","    print(f'Parameter name: {param_name:42} value = {param.item()}')"],"metadata":{"id":"WK-tGZohRlRR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Notice that the parameters are referencing \"raw\" values such as raw_noise, raw_outputscale, and raw_lengthscale. This is because, in GPyTorch, parameters can be constrained (e.g., all of the previous parameters referenced must be positive) such that it uses \"constraint functions\" to enforce these constraints directly. This leads to a critical difference between **raw parameters** and **actual parameters**. For example, we can extract the raw_outputscale and its associated constraint and check that, when it is run through the transform and then the inverse transform, it leads back to the raw value."],"metadata":{"id":"S7HiSJdKR6DT"}},{"cell_type":"code","source":["raw_outputscale = model.covar_module.raw_outputscale\n","constraint = model.covar_module.raw_outputscale_constraint\n","print(f'Raw outputscale: {raw_outputscale.detach().item()}')\n","print(f'Actual (transformed) outputscale: {constraint.transform(raw_outputscale).detach().item()}')\n","print(f'Is inverse_transform(transform(raw_outputscale)) == raw_outputscale? {torch.equal(constraint.inverse_transform(constraint.transform(raw_outputscale)), raw_outputscale)}')"],"metadata":{"id":"jbmLbW247Zk4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Since dealing with raw parameter values is often annoying, most of the built-in GPyTorch modules that define raw parameters define convenience getters and setters for dealing with transformed values directly.\n"],"metadata":{"id":"pJpEaypU8pPE"}},{"cell_type":"code","source":["# recreate model to reset outputscale\n","model = MyGP(train_x, train_y, likelihood)\n","\n","# convenient way of getting true outputscale\n","print(f'Actual outputscale: {model.covar_module.outputscale}')\n","\n","# convenient way of setting true outputscale\n","model.covar_module.outputscale = 2.\n","print(f'Actual outputscale after setting: {model.covar_module.outputscale}')"],"metadata":{"id":"bTUZDFSj86EF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model modes\n","\n","Following the standard approach in PyTorch, the ExactGP (and other GPyTorch modules) has a ```.train()``` and ```.eval()``` mode.\n","* ```.train()```: This mode should be used when optimizing the hyperparameters of the model.\n","* ```.eval()```: This mode should be used for computing predictions from the model posterior."],"metadata":{"id":"FtyL576LOq-e"}},{"cell_type":"markdown","source":["## 2.2 Training a Model\n","\n","Below, we show how to learn/train the hyperparameters a GP using the \"Type-II maximum likelihood estimator\", which corresponds to maximizing the marginal log likelihood function. Just like in PyTorch, the core training loop is written by the user. This means we can use all of the available optimization algorithms from ```torch.optim```. Additionally, all of the trainable parameters of the model should be of type ```torch.nn.Parameter``` and should be attached to the named parameters in the model (as shown in a previous cell)."],"metadata":{"id":"rNdH4z5t_0NK"}},{"cell_type":"markdown","source":["### Adam\n","\n","The boilerplate code often works quite well, which has the same basic components as the standard PyTorch training loop:\n","1. Zero all parameter gradients\n","2. Call the model and compute the loss\n","3. Call backward on the loss to fill in gradients (autograd)\n","4. Take a step on the optimizer\n","Let's see how this works below for our sin example using the [Adam](https://arxiv.org/pdf/1412.6980) optimizer."],"metadata":{"id":"EX9SmnaTT0c8"}},{"cell_type":"code","source":["# recreate model and likelihood to reset hyperparameters\n","likelihood = gpytorch.likelihoods.GaussianLikelihood()\n","model = MyGP(train_x, train_y, likelihood)\n","\n","# set the model into train mode\n","model.train()\n","\n","# \"loss\" for GPs - the marginal log likelihood\n","mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n","\n","# use the adam optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n","\n","# loop for some number training iterations\n","training_iter = 50\n","for i in range(training_iter):\n","    # zero gradients from previous iteration\n","    optimizer.zero_grad()\n","    # calc output from model\n","    output = model(train_x)\n","    # calc loss and backprop gradients\n","    loss = -mll(output, train_y)\n","    loss.backward()\n","    # print the current loss\n","    print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n","        i + 1, training_iter, loss.item(),\n","        model.covar_module.base_kernel.lengthscale.item(),\n","        model.likelihood.noise.item()\n","    ))\n","    # take a step in the optimizer\n","    optimizer.step()"],"metadata":{"id":"DsYSiSbN-jhN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### L-BFGS\n","\n","Just to illustrate the flexibility of the optimization process, we also show how you can replace the first-order Adam method with a quasi-Newton method such as the [L-BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS) optimization algorithm. L-BFGS creates an estimate of the inverse hessian matrix for the objective function using a history of past updates. Due to its more complicated structure, the training loop does need to be modified some as shown in the code block below. However, notice how we can easily make such modifications by building on all the great tooling developed over many years in PyTorch."],"metadata":{"id":"mskrZaVqTl9w"}},{"cell_type":"code","source":["# recreate model and likelihood to reset hyperparameters\n","likelihood = gpytorch.likelihoods.GaussianLikelihood()\n","model = MyGP(train_x, train_y, likelihood)\n","\n","# set the model into train mode\n","model.train()\n","\n","# \"loss\" for GPs - the marginal log likelihood\n","mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n","\n","# define optimizer -- now LBFGS\n","lbfgs = torch.optim.LBFGS(model.parameters(), history_size=20, max_iter=5, line_search_fn=\"strong_wolfe\")\n","\n","# define a closure needed for LBFGS\n","def closure():\n","    lbfgs.zero_grad()\n","    output = model(train_x)\n","    loss = -mll(output, train_y)\n","    loss.backward()\n","    return loss\n","\n","# loop for some number training iterations\n","training_iter = 10\n","for i in range(training_iter):\n","    # calculate the loss for monitoring\n","    with torch.no_grad():\n","        output = model(train_x)\n","        loss = -mll(output, train_y)\n","    # print the current loss\n","    print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n","        i + 1, training_iter, loss.item(),\n","        model.covar_module.base_kernel.lengthscale.item(),\n","        model.likelihood.noise.item()\n","    ))\n","    # take step in lbfgs using closure\n","    lbfgs.step(closure)"],"metadata":{"id":"B0vLJwfeQOlh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Notice that the best-found parameters are quite similar between Adam and L-BFGS; however, there are some minor differences. Technically, the loss is slightly better with the settings found by L-BFGS (-0.609 vs. -0.607). It is also interesting to note that L-BFGS converges to stable values in many fewer iterations, though the cost per iteration is a bit higher than that of Adam."],"metadata":{"id":"KLGlt4RFU9hO"}},{"cell_type":"markdown","source":["## 2.3 Making Model Predictions\n","\n","To make predictions with the model, all we need to do is put the model and likelihood into ```.eval()``` mode and then call these modules on relevant test data. We perform those computations and then plot the resulting model fit in the cells below."],"metadata":{"id":"RS9h6VV7PG8y"}},{"cell_type":"markdown","source":["### Get Test Predictions\n","\n","Just as a user-defined GP model returns a ```MultivariateNormal``` object with the prior mean and covariance from the ```forward``` method, a trained GP model put into ```.eval()``` mode returns a ```MultivariateNormal``` containing the posterior mean and covariance. Thus, we can get the predictive mean, variance, covariance, and draw samples from the GP at the test points using the calls shown in the following cell block:\n"],"metadata":{"id":"3jEMjrBvYn7c"}},{"cell_type":"code","source":["# get into evaluation (predictive posterior) mode\n","model.eval()\n","likelihood.eval()\n","\n","# generate some test points\n","test_x = torch.linspace(0, 1, 101)\n","\n","# evaluate the model to get f predictions and likelihood to get y predictions\n","#   The gpytorch.settings.fast_pred_var context is not needed, but yields faster predictive distributions using LOVE.\n","with torch.no_grad(), gpytorch.settings.fast_pred_var():\n","    f_preds = model(test_x)\n","    y_preds = likelihood(model(test_x))\n","\n","# extract the mean, variance, and covariance information from the posterior distribution\n","f_mean = f_preds.mean\n","f_var = f_preds.variance\n","f_covar = f_preds.covariance_matrix\n","\n","# generate some random samples from the posterior\n","f_samples = f_preds.sample(sample_shape=torch.Size([10]))"],"metadata":{"id":"WbdKPXVaWMzS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Plot the Model Fit\n","\n","Below, we plot the mean and confidence region of the (posterior) GP model. We use the ```.confidence_region()``` method that automatically returns 2 standard deviations above and below the mean. Note that we plot the confidence region for both the function $f(x)$ and the observations of the function $y = f(x) + \\epsilon$, with the latter being computed by combining the model predictions with the likelihood.\n"],"metadata":{"id":"1Ldh7F1bYvXk"}},{"cell_type":"code","source":["# create a plot\n","fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n","# get upper and lower confidence bounds\n","lower, upper = f_preds.confidence_region()\n","lower_obs, upper_obs = y_preds.confidence_region()\n","# Plot training data as black stars\n","ax.plot(train_x.numpy(), train_y.numpy(), 'k*')\n","# Plot predictive means as blue line\n","ax.plot(test_x.numpy(), f_mean.numpy(), 'b')\n","# shade between the lower and upper confidence bounds for function f\n","ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.25)\n","# shade between the lower and upper confidence bounds for outputs y\n","ax.fill_between(test_x.numpy(), lower_obs.numpy(), upper_obs.numpy(), alpha=0.25)\n","ax.set_ylim([-2, 2])\n","ax.legend(['observed data', 'mean', 'confidence for f', 'confidence for y'])\n","plt.xlabel('input, x')\n","plt.ylabel('output, f(x)')\n","plt.rcParams.update({'font.size':14});"],"metadata":{"id":"QONsgsWGV8L1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.4 Custom Kernels\n","\n","A powerful feature of GPyTorch is the ability to create custom kernel functions, as long as they can be implemented with torch functions (that allow for easy automatic differentiation). We will explore implementing the Brownian motion kernel that has the following form:\n","\n","$$\n","k(x, x') = \\sigma^2 \\min(x, x')\n","$$\n","\n","This kernel is particularly useful in finance (e.g., modeling stock prices), physics (e.g., particle trajectories), and other areas where the modeled process follows a Brownian motion. As such, it is a popular covariance function for GPs when modeling processes that exhibit continuous, non-stationary, and unpredictable paths over time ($x$ normally represents time in these applications).\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"iYGIUzfv_xjQ"}},{"cell_type":"markdown","source":["### Exercise\n","\n","**TASK:** Your goal is to implement this kernel (which does not come natively in GPyTorch) and apply it to a simulated Brownian motion trajectory. You will need to attach a hyperparameter that corresponds to the scale $\\sigma^2$. To help you, I have provided several of the components needed in the code block below. This code is based on the [custom kernel tutorial](https://docs.gpytorch.ai/en/v1.6.0/examples/00_Basic_Usage/Implementing_a_custom_Kernel.html?highlight=custom%20kernel) provided in the GPyTorch documentation. You will see that I registered a parameter ```raw_scale``` that is the \"raw\" form of the parameter as well as a positive constraint ```raw_scale_constraint``` that can be used to transform from a raw to the actual parameter value. The main missing parts are related to the forward evaluation of the kernel. Note that this will require you to compute pairwise minimums across all the combinations of $x$ and $x'$ (fed in batchwise as $n \\times 1$ matrices since $x$ must be 1d). You can do this using the standard ```torch.min()```, though you need to be careful with how call it.\n","* NOTE: You should look through all components of the code carefully to make sure you understand how the steps work."],"metadata":{"id":"yixgKwVVPvhO"}},{"cell_type":"code","source":["##############################################################\n","# Only modify this part of code block; replace FILL IN parts\n","##############################################################\n","# Create Brownian Motion Kernel\n","class BrownianMotionKernel(gpytorch.kernels.Kernel):\n","    # the brownian kernel is not stationary\n","    is_stationary = False\n","\n","    # we will register the scale parameter when initializing the kernel\n","    def __init__(self, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        # register the raw scale parameter\n","        self.register_parameter(name='raw_scale', parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, 1, 1)))\n","\n","        # set the parameter constraint to be positive\n","        scale_constraint = Positive()\n","\n","        # register the constraint\n","        self.register_constraint(\"raw_scale\", scale_constraint)\n","\n","    # now set up the 'actual' paramter\n","    @property\n","    def scale(self):\n","        # when accessing the parameter, apply the constraint transform\n","        return self.raw_scale_constraint.transform(self.raw_scale)\n","\n","    @scale.setter\n","    def scale(self, value):\n","        return self._set_scale(value)\n","\n","    def _set_scale(self, value):\n","        if not torch.is_tensor(value):\n","            value = torch.as_tensor(value).to(self.raw_scale)\n","        # when setting the paramater, transform the actual value to a raw one by applying the inverse transform\n","        self.initialize(raw_scale=self.raw_scale_constraint.inverse_transform(value))\n","\n","    # this is the kernel function\n","    def forward(self, x1, x2, **params):\n","        # check statements\n","        assert x1.shape[-1] == 1 and x2.shape[-1] == 1 # only works for 1d input\n","        assert torch.all(x1 >= 0) and torch.all(x2 >= 0) # negative times are not currently supported\n","\n","        # compute the pairwise minimums\n","        ### FILL IN\n","\n","        # make sure values are positive\n","        ### FILL IN\n","\n","        # return the scaled covariance matrix\n","        ### FILL IN\n","##############################################################\n","##############################################################\n","\n","# Generate training data with fixed seed\n","torch.manual_seed(42)\n","train_x = torch.linspace(0, 1, 100)\n","train_y = torch.cumsum(0.05*torch.randn(train_x.size()), dim=0)  # Simulate Brownian motion data\n","\n","# Define GP model with Brownian motion kernel\n","class GPModel(gpytorch.models.ExactGP):\n","    def __init__(self, train_x, train_y, likelihood):\n","        super(GPModel, self).__init__(train_x, train_y, likelihood)\n","        self.mean_module = gpytorch.means.ConstantMean()\n","        self.covar_module = BrownianMotionKernel()\n","\n","    def forward(self, x):\n","        mean_x = self.mean_module(x)\n","        covar_x = self.covar_module(x)\n","        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n","\n","# Set up the likelihood and model [here we use a fixed noise model]\n","likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(noise = 1e-4*torch.ones_like(train_x))\n","model = GPModel(train_x, train_y, likelihood)\n","\n","# Train the model\n","model.train()\n","\n","# Use the Adam optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n","\n","# \"Loss\" for GPs is the marginal log likelihood\n","mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n","\n","# Training loop\n","training_iter = 200\n","for i in range(training_iter):\n","    optimizer.zero_grad()\n","    output = model(train_x)\n","    loss = -mll(output, train_y)\n","    loss.backward()\n","    optimizer.step()\n","\n","# Convert model to eval mode to make predictions on test data\n","model.eval()\n","\n","# Make predictions\n","test_x = torch.linspace(0, 1.6, 101)\n","with torch.no_grad(), gpytorch.settings.fast_pred_var():\n","    f_preds = model(test_x)\n","\n","# Make plot of predictions\n","with torch.no_grad():\n","    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n","    # get upper and lower confidence bounds\n","    lower, upper = f_preds.confidence_region()\n","    # Plot training data as black stars\n","    ax.plot(train_x.numpy(), train_y.numpy(), 'k*')\n","    # Plot predictive means as blue line\n","    ax.plot(test_x.numpy(), f_preds.mean.numpy(), 'b')\n","    # shade between the lower and upper confidence bounds for function f\n","    ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.25)\n","    ax.legend(['observed data', 'mean', 'confidence'], loc='upper left')\n","    plt.xlabel('input, x')\n","    plt.ylabel('output, f(x)')\n","    plt.rcParams.update({'font.size':14});"],"metadata":{"id":"m6bqW-bnlfFw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Solution"],"metadata":{"id":"kJIrBNlrPxHp"}},{"cell_type":"code","source":["# Create Brownian Motion Kernel\n","class BrownianMotionKernel(gpytorch.kernels.Kernel):\n","    # the brownian kernel is not stationary\n","    is_stationary = False\n","\n","    # we will register the scale parameter when initializing the kernel\n","    def __init__(self, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        # register the raw scale parameter\n","        self.register_parameter(name='raw_scale', parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, 1, 1)))\n","\n","        # set the parameter constraint to be positive\n","        scale_constraint = Positive()\n","\n","        # register the constraint\n","        self.register_constraint(\"raw_scale\", scale_constraint)\n","\n","    # now set up the 'actual' paramter\n","    @property\n","    def scale(self):\n","        # when accessing the parameter, apply the constraint transform\n","        return self.raw_scale_constraint.transform(self.raw_scale)\n","\n","    @scale.setter\n","    def scale(self, value):\n","        return self._set_scale(value)\n","\n","    def _set_scale(self, value):\n","        if not torch.is_tensor(value):\n","            value = torch.as_tensor(value).to(self.raw_scale)\n","        # when setting the paramater, transform the actual value to a raw one by applying the inverse transform\n","        self.initialize(raw_scale=self.raw_scale_constraint.inverse_transform(value))\n","\n","    # this is the kernel function\n","    def forward(self, x1, x2, **params):\n","        # check statements\n","        assert x1.shape[-1] == 1 and x2.shape[-1] == 1 # only works for 1d input\n","        assert torch.all(x1 >= 0) and torch.all(x2 >= 0) # negative times are not currently supported\n","\n","        # compute the pairwise minimums\n","        covar_matrix = torch.min(x1, x2.T)\n","\n","        # make sure values are positive\n","        covar_matrix.where(covar_matrix <= 0, torch.as_tensor(1e-6))\n","\n","        # return the scaled covariance matrix\n","        return self.scale * covar_matrix\n","\n","# Generate training data with fixed seed\n","torch.manual_seed(42)\n","train_x = torch.linspace(0, 1, 100)\n","train_y = torch.cumsum(0.05*torch.randn(train_x.size()), dim=0)  # Simulate Brownian motion data\n","\n","# Define GP model with Brownian motion kernel\n","class GPModel(gpytorch.models.ExactGP):\n","    def __init__(self, train_x, train_y, likelihood):\n","        super(GPModel, self).__init__(train_x, train_y, likelihood)\n","        self.mean_module = gpytorch.means.ConstantMean()\n","        self.covar_module = BrownianMotionKernel()\n","\n","    def forward(self, x):\n","        mean_x = self.mean_module(x)\n","        covar_x = self.covar_module(x)\n","        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n","\n","# Set up the likelihood and model [here we use a fixed noise model]\n","likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(noise = 1e-4*torch.ones_like(train_x))\n","model = GPModel(train_x, train_y, likelihood)\n","\n","# Train the model\n","model.train()\n","\n","# Use the Adam optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n","\n","# \"Loss\" for GPs is the marginal log likelihood\n","mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n","\n","# Training loop\n","training_iter = 200\n","for i in range(training_iter):\n","    optimizer.zero_grad()\n","    output = model(train_x)\n","    loss = -mll(output, train_y)\n","    loss.backward()\n","    optimizer.step()\n","\n","# Convert model to eval mode to make predictions on test data\n","model.eval()\n","\n","# Make predictions\n","test_x = torch.linspace(0, 1.6, 101)\n","with torch.no_grad(), gpytorch.settings.fast_pred_var():\n","    f_preds = model(test_x)\n","\n","# Make plot of predictions\n","with torch.no_grad():\n","    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n","    # get upper and lower confidence bounds\n","    lower, upper = f_preds.confidence_region()\n","    # Plot training data as black stars\n","    ax.plot(train_x.numpy(), train_y.numpy(), 'k*')\n","    # Plot predictive means as blue line\n","    ax.plot(test_x.numpy(), f_preds.mean.numpy(), 'b')\n","    # shade between the lower and upper confidence bounds for function f\n","    ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.25)\n","    ax.legend(['observed data', 'mean', 'confidence'], loc='upper left')\n","    plt.xlabel('input, x')\n","    plt.ylabel('output, f(x)')\n","    plt.rcParams.update({'font.size':14});"],"metadata":{"id":"aU9HNzew81YR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. GP Regression: Modeling CO$_2$ Levels at the Mauna Loa Observatory\n","\n","In this section, we want to use the well-known atmospheric CO$_2$ modeling problem to reinforce the different concepts we learned in the previous sections. The [data](https://cir.nii.ac.jp/crid/1571980076132477696) consists of monthly average values of the CO$_2$ concentration in parts per million by volume (ppmv). This data was collected at the Mauna Loa Observatory, Hawaii, between 1958 and 2003 (with some missing values). We want to model this data using a GP with a relatively complex kernel -- the need for a complex kernel will become apparent shortly.\n"],"metadata":{"id":"TOPJ-47caAHO"}},{"cell_type":"markdown","source":["## Exercise\n","\n","**TASK:** The goal of this exercise is effectively to reproduce the sklearn implementation of this problem, which can be found [here](https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_co2.html). We will first prepare the data in a usable format. Then, you will attempt to train a GP model with a relatively simple kernel wherein you will see that the fit is not particularly good. By implementing the more complex kernel suggested in Section 5.4.3. of the [Gaussian Processes for Machine Learning](https://gaussianprocess.org/gpml/chapters/RW.pdf) book, we can achieve a much better fit."],"metadata":{"id":"UbPAzjKkCzFi"}},{"cell_type":"markdown","source":["### Prepare Train and Test Data\n","\n","We first just load the data. We will use all available observations for training, which includes times from around 1958 to 2001."],"metadata":{"id":"-S4oZ_cfDN2u"}},{"cell_type":"code","source":["# create a function that fetches data and processes it as needed\n","def load_mauna_loa_atmospheric_co2():\n","    ml_data = fetch_openml(data_id=41187, as_frame=False)\n","    months = []\n","    ppmv_sums = []\n","    counts = []\n","\n","    y = ml_data.data[:, 0]\n","    m = ml_data.data[:, 1]\n","    month_float = y + (m - 1) / 12\n","    ppmvs = ml_data.target\n","\n","    for month, ppmv in zip(month_float, ppmvs):\n","        if not months or month != months[-1]:\n","            months.append(month)\n","            ppmv_sums.append(ppmv)\n","            counts.append(1)\n","        else:\n","            # aggregate monthly sum to produce average\n","            ppmv_sums[-1] += ppmv\n","            counts[-1] += 1\n","\n","    months = np.array(months).reshape(-1, 1)\n","    avg_ppmvs = np.array(ppmv_sums) / counts\n","    return months, avg_ppmvs\n","\n","# function to convert numpy arrays to torch tensors\n","def to_torch(V):\n","    return torch.tensor(V).flatten().double()\n","\n","# call data loading function to get raw data\n","X_raw, Y_raw = load_mauna_loa_atmospheric_co2()\n","\n","# split data into train and test sets and convert to torch\n","train_x = to_torch(X_raw)\n","train_y = to_torch(Y_raw)\n","\n","# let's just make a simple plot, so we can visualize the data\n","f, ax = plt.subplots(1, 1, figsize=(10, 4))\n","ax.scatter(train_x, train_y, 5, 'b')\n","plt.xlabel('Year')\n","plt.ylabel('CO$_2$ Concentration (ppm)')\n","plt.grid(True)\n","plt.rcParams.update({'font.size':14})"],"metadata":{"id":"pselnt0GDMgf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We immediately notice some important things about this data:\n","* it has a long term rising trend;\n","* it has a pronounced seasonal variation;\n","* and it has some smaller short term irregularities.\n","\n","Each of these types of behaviors can be captured with separate covariance functions (kernels) that can be combined together to deal with these individual properties."],"metadata":{"id":"6NEcboRZcA7w"}},{"cell_type":"markdown","source":["### Train and Test GP Model\n","\n","I recommend tackling the training and testing component in 2 separate parts.\n","1. First, just write the individual sections of the code (the GP model, the training loop, and the prediction plot) using a simple RBF kernel. You can then investigate the performance and see what MLL it achieves.\n","2. Second, you can then easily update your code developed in the first part by replacing the ```covar_module``` attached to the GP using the suggested approach in the [sklearn example](https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_co2.html). Note that you need to stick very close to their suggested form including using the same initial guesses for the hyperparameters for your model to achieve good performance.\n","  * You can scroll down to the \"Design the proper kernel\" section to see the details of the kernel, which is the sum of 4 individual kernels. It has a form like: $k_\\text{long-term}(x, x') + k_\\text{seasonal}(x, x') + k_\\text{irregularities}(x, x') + k_\\text{noise}(x, x')$.\n","  * Compare the MLL you get with this more complex kernel to the standard RBF kernel, which one would you prefer?\n","\n","TIPS: Here are some important tips you should follow when setting up the code in Step 1.\n","* You do not need to normalize the input or output values, as that will change the meaning of the hyperparameters (and you will need to derive updated settings in the scaled space). However, you do need to subtract the mean value from the output values before training, i.e., calculate ```y_mean = train_y.mean()``` and subtract it from ```train_y``` before passing to the GP model.\n","* The loss landscape is very complex and thus I found that most of the default optimizers failed. As such, I recommend using the ```fit_gpytorch_mll``` function from BoTorch, which was already loaded with the imports at the beginning of the notebook. All you need to do to use this function is to pass the evaluated the MLL evaluated from the model in ```.train()``` mode. It will automatically optimize the hyperparameters using a slightly more sophisticated version of the L-BFGS algorithm.\n","* You want to run your test predictions from 1958 to 2025 broken into 1001 increments."],"metadata":{"id":"eznP8ZL5DU99"}},{"cell_type":"code","source":["### put your solution here\n"],"metadata":{"id":"8SKgZYgIc2Nb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Go back and modify the kernel and see if you can get better performance. I recommend trying the following kernels: (i) spectral mixture (need to be careful with initialization), (ii)"],"metadata":{"id":"-1kluAyODv8Q"}},{"cell_type":"markdown","source":["## Solution"],"metadata":{"id":"JmK9ib8rC1ce"}},{"cell_type":"code","source":["### This code block implements both the RBF and the Complex kernel.\n","### You can switch between them setting \"use_complex\" to True or False\n","\n","# set flag for a complex kernel or not\n","#   False implies RBF while True implies the additive complex kernel\n","use_complex = True\n","\n","# calculate mean of training outputs\n","y_mean = train_y.mean()\n","\n","# define exact GP model\n","class GPModel(gpytorch.models.ExactGP):\n","    def __init__(self, train_x, train_y, likelihood, kernel):\n","        super(GPModel, self).__init__(train_x, train_y, likelihood)\n","        self.mean_module = gpytorch.means.ZeroMean()\n","        self.covar_module = kernel\n","\n","    def forward(self, x):\n","        mean_x = self.mean_module(x)\n","        covar_x = self.covar_module(x)\n","        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n","\n","# define a function that generates the complex kernel\n","def complex_kernel():\n","    #### long term\n","    k_long_term_RBF = gpytorch.kernels.RBFKernel()\n","    k_long_term_RBF.lengthscale = 50.0\n","\n","    k_long_term = gpytorch.kernels.ScaleKernel(k_long_term_RBF)\n","    k_long_term.outputscale = 50.0**2\n","\n","    #### seasonal\n","    k_seasonal_RBF = gpytorch.kernels.RBFKernel()\n","    k_seasonal_RBF.lengthscale = 100.0\n","\n","    k_seasonal_periodic = gpytorch.kernels.PeriodicKernel()\n","    k_seasonal_periodic.period_length = 1.0\n","    k_seasonal_periodic.lengthscale = 1.0\n","\n","    k_seasonal = gpytorch.kernels.ScaleKernel(k_seasonal_RBF * k_seasonal_periodic)\n","    k_seasonal.outputscale = 4.0\n","\n","    ### medium term\n","    k_medium_RQ = gpytorch.kernels.RQKernel()\n","    k_medium_RQ.alpha = 1.0\n","    k_medium_RQ.lengthscale = 1.0\n","\n","    k_medium = gpytorch.kernels.ScaleKernel(k_medium_RQ)\n","    k_medium.outputscale = 0.5**2\n","\n","    #### noise term\n","    k_noise_RBF = gpytorch.kernels.RBFKernel()\n","    k_noise_RBF.lengthscale = 0.1\n","\n","    k_noise = gpytorch.kernels.ScaleKernel(k_noise_RBF)\n","    k_noise.outputscale = 0.1**2\n","\n","    return k_long_term + k_seasonal + k_medium + k_noise\n","\n","# get the kernel we will be using\n","if use_complex:\n","    kernel = complex_kernel()\n","else:\n","    kernel = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n","\n","# define the model and likelihood function\n","likelihood = gpytorch.likelihoods.GaussianLikelihood()\n","model = GPModel(train_x, train_y - y_mean, likelihood, kernel)\n","\n","# make sure the model is in train mode\n","model.train()\n","\n","# get \"loss\" for GPs - the marginal log likelihood\n","mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n","\n","# call the optimization wrapper in BoTorch for training GP models\n","fit_gpytorch_mll(mll)\n","\n","# evaluate and print the marginal log likelihood\n","model.train()\n","with torch.no_grad():\n","    output = model(train_x)\n","    loss = -mll(output, train_y - y_mean)\n","    print('MLL for optimal hyperparameters: %.3f' % -loss.item())\n","\n","# put model into eval mode to start making predictions\n","model.eval()\n","\n","# make predictions\n","test_x = torch.linspace(1958, 2025, 1001)\n","with torch.no_grad(), gpytorch.settings.fast_pred_var():\n","    f_preds = model(test_x)\n","\n","# plot predictions\n","with torch.no_grad():\n","    # make plot\n","    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n","    # get upper and lower confidence bounds\n","    lower, upper = f_preds.confidence_region()\n","    # add back mean value to get correct bounds\n","    lower, upper = lower + y_mean, upper + y_mean\n","    # plot training data as blue dots\n","    ax.scatter(train_x, train_y, 5, 'b')\n","    # plot predictive means as a gray line\n","    f_mean = f_preds.mean + y_mean\n","    ax.plot(test_x.numpy(), f_mean.numpy(), 'gray')\n","    # shade between the lower and upper confidence bounds for function f\n","    ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.25, color='gray')\n","    # add a legend and labels\n","    ax.legend(['observed data', 'mean', 'confidence'], loc='upper left')\n","    plt.xlabel('input, x')\n","    plt.ylabel('output, f(x)')\n","    plt.rcParams.update({'font.size':14});"],"metadata":{"id":"VmaPgcPMEavZ"},"execution_count":null,"outputs":[]}]}